import logging
import copy

from unlanedet.checkpoint import Checkpointer,BestCheckPointer
from unlanedet.config import LazyConfig, instantiate
from unlanedet.engine import (
    AMPTrainer,
    SimpleTrainer,
    default_argument_parser,
    default_setup,
    default_writers,
    hooks,
    launch,
)
from unlanedet.engine.defaults import create_ddp_model
from unlanedet.evaluation import inference_on_dataset, print_csv_format
from unlanedet.utils import comm


logger = logging.getLogger("unlanedet")


def do_test(cfg, model):
    if "evaluator" in cfg.dataloader:
        # Convert to SyncBatchNorm if in distributed mode and not already converted
        if comm.get_world_size() > 1:
            import torch
            # Check if model already has SyncBatchNorm modules
            has_sync_bn = any(isinstance(module, torch.nn.SyncBatchNorm) 
                            for module in model.modules())
            if not has_sync_bn:
                logger.info("Converting model to use SyncBatchNorm for evaluation")
                model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)
        
        ret = inference_on_dataset(
            model,
            instantiate(cfg.dataloader.test),
            instantiate(cfg.dataloader.evaluator),
        )
        print_csv_format(ret)
        return ret


def do_train(args, cfg):
    """
    Args:
        cfg: an object with the following attributes:
            model: instantiate to a module
            dataloader.{train,test}: instantiate to dataloaders
            dataloader.evaluator: instantiate to evaluator for test set
            optimizer: instantaite to an optimizer
            lr_multiplier: instantiate to a fvcore scheduler
            train: other misc config defined in `configs/common/train.py`, including:
                output_dir (str)
                init_checkpoint (str)
                amp.enabled (bool)
                max_iter (int)
                eval_period, log_period (int)
                device (str)
                checkpointer (dict)
                ddp (dict)
    """
    model = instantiate(cfg.model)
    logger = logging.getLogger("unlanedet")
    logger.info("Model:\n{}".format(model))
    model.to(cfg.train.device)

    if comm.get_world_size() > 1:
        import torch
        logger.info("Converting model to use SyncBatchNorm")
        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)
    cfg_copy = copy.deepcopy(cfg)
    cfg_copy.optimizer.params.model = model
    optim = instantiate(cfg_copy.optimizer)

    train_loader = instantiate(cfg.dataloader.train)

    model = create_ddp_model(model, **cfg.train.ddp)
    trainer = (AMPTrainer if cfg.train.amp.enabled else SimpleTrainer)(model, train_loader, optim)
    checkpointer = BestCheckPointer(
        model,
        cfg.train.output_dir,
        trainer=trainer,
    )
    trainer.register_hooks(
        [
            hooks.IterationTimer(),
            hooks.SetEpochHook(train_loader.sampler),
            hooks.LRScheduler(scheduler=instantiate(cfg.lr_multiplier)),
            (
                hooks.PeriodicCheckpointer(checkpointer, **cfg.train.checkpointer)
                if comm.is_main_process()
                else None
            ),
            hooks.EvalHook(cfg.train.eval_period, lambda: do_test(cfg, model)),
            # Save the model achieving the best result.
            (
                hooks.BestCheckpointer(checkpointer=checkpointer,eval_period=cfg.train.eval_period,val_metric=cfg.dataloader.evaluator.metric)
                if comm.is_main_process()
                else None
            ),
            (
                hooks.PeriodicWriter(
                    default_writers(cfg.train.output_dir, cfg.train.max_iter),
                    period=cfg.train.log_period,
                )
                if comm.is_main_process()
                else None
            ),
        ]
    )

    checkpointer.resume_or_load(cfg.train.init_checkpoint, resume=args.resume)
    if args.resume and checkpointer.has_checkpoint():
        # The checkpoint stores the training iteration that just finished, thus we start
        # at the next iteration
        start_iter = trainer.iter + 1
    else:
        start_iter = 0
    trainer.train(start_iter, cfg.train.max_iter)


def main(args):
    cfg = LazyConfig.load(args.config_file)
    cfg = LazyConfig.apply_overrides(cfg, args.opts)
    default_setup(cfg, args)

    if args.eval_only:
        model = instantiate(cfg.model)
        model.to(cfg.train.device)
        model = create_ddp_model(model)
        Checkpointer(model).load(cfg.train.init_checkpoint)
        print(do_test(cfg, model))
    else:
        do_train(args, cfg)


def invoke_main() -> None:
    args = default_argument_parser().parse_args()
    launch(
        main,
        args.num_gpus,
        num_machines=args.num_machines,
        machine_rank=args.machine_rank,
        dist_url=args.dist_url,
        args=(args,),
    )


if __name__ == "__main__":
    invoke_main()  # pragma: no cover
